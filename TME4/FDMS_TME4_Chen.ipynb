{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 - Filtrage Collaboratif\n",
    "\n",
    "L'objectif de ce TME est d'implémenter des méthodes de filtrage collaboratif.\n",
    "\n",
    "On utilisera les données Movie Lens 100k et Movie Lens 1M qui correspondent à deux bases de données de notes attribués par un ensemble d'utilisateurs sur des films, contenant respectivement 100 000 et 1 millions d'entrés.\n",
    "\n",
    "On s'intéressera en premier lieu à un modèle de factorisation de matrice simple sans biais, puis on incluera dans le modèle un biais par utilisateur et par film. Enfin, on implémentera un modèle tenant compte du fait que ces biais peuvent évoluer dans le temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "Les fonctions pour charger les bases Movie Lens 100k et Movie Lens 1M.\n",
    "On récupère un dictionnaire pour les scores et un dictionnaire pour les dates.\n",
    "Le première index de ces dictionnaires est l'identifiant de l'utilisateur, et le second les films notés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadMovieLens(path='./data100k'):\n",
    "    # Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/u.item'):\n",
    "        (id,title)=line.split('|')[0:2]\n",
    "        movies[id]=title\n",
    "    # Load data\n",
    "    prefs={} # Un dictionnaire User > Item > Rating\n",
    "    times={} # Un dictionnaire User > Item > Timestamps\n",
    "    for line in open(path+'/u.data'):\n",
    "        (user,movieid,rating,ts)=line.split('\\t')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "        times.setdefault(user,{})\n",
    "        times[user][movies[movieid]]=float(ts)\n",
    "    return prefs, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMovieLens1M(path='./data1m'):\n",
    "    # Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/movies.dat'):\n",
    "        id,title=line.split('::')[0:2]\n",
    "        movies[id]=title\n",
    "    # Load data\n",
    "    prefs={}\n",
    "    times={}\n",
    "    for line in open(path+'/ratings.dat'):\n",
    "        (user,movieid,rating,ts)=line.split('::')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "        times.setdefault(user,{})\n",
    "        times[user][movies[movieid]]=float(ts)\n",
    "    return prefs, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentations des données\n",
    "\n",
    "Les matrices des scores Utilisateurs/Films sont des matrices de grandes dimensions mais sparses.\n",
    "Afin de les manipuler efficacement, on emploiera 3 représentations différentes en même temps:\n",
    "- Le dictionnaire des scores par utilisateurs: User > Item > Value\n",
    "- Le dictionnaire des scores par films: Item > User > Value \n",
    "- La liste des triplets [User, Item, Value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recupère une représentation des données sous la forme triplets [user, item, value] a partir d'un dictionnaire [User > item > value]\n",
    "def getCouplesUsersItems(data):\n",
    "    couples = []\n",
    "    for u in data.keys():\n",
    "        for i in data[u].keys():\n",
    "            couples.append([u,i,data[u][i]])\n",
    "    return couples\n",
    "\n",
    "# Construit le dictionnaire des utilisateurs a partir des triplets [user, item, note]\n",
    "def buildUsersDict(couples):\n",
    "    dicUsers = {}\n",
    "    for c in couples:\n",
    "        if not c[0] in dicUsers.keys():\n",
    "            dicUsers[c[0]] = {}\n",
    "        dicUsers[c[0]][c[1]] = float(c[2])\n",
    "    return dicUsers\n",
    "\n",
    "# Construit le dictionnaire des objets a partir des triplets [user, item, note]\n",
    "def buildItemsDict(couples):\n",
    "    dicItems = {}\n",
    "    for c in couples:\n",
    "        if not c[1] in dicItems:\n",
    "            dicItems[c[1]] = {}\n",
    "        dicItems[c[1]][c[0]] = float(c[2])\n",
    "    return dicItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données de temps\n",
    "\n",
    "Afin d'exploiter les données temporelles, on discrétise le temps en bins et on construit un vecteur qui a chaque triplets [user, item, value] associe le bins temporel correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTimeBins(couples, timedic, nbins):\n",
    "    timestamps = np.zeros(len(couples))\n",
    "    for i,c in enumerate(couples):\n",
    "        timestamps[i] = timedic[c[0]][c[1]]\n",
    "    time_bins = np.linspace(np.min(timestamps), np.max(timestamps), nbins+1)\n",
    "    times = np.zeros(len(couples),int)\n",
    "    for i in xrange(1,len(time_bins)):\n",
    "        times = times + (timestamps > time_bins[i])\n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données en Train / Test\n",
    "\n",
    "Pour pouvoir séparer les données en ensembles de Train et de Test, on utilisera la liste des triplets [User, Item, Scores]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split l'ensemble des triplets [user, item, note] en testProp% données de test et (1 - testProp) données de train\n",
    "def splitTrainTest(couples,testProp):\n",
    "    perm = np.random.permutation(couples)\n",
    "    splitIndex = int(testProp * len(couples))\n",
    "    return perm[splitIndex:], perm[:splitIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles\n",
    "\n",
    "On implémente ici les différents modèles. Les baselines prédisent simplement la note moyenne pour un utilisateur (ou pour un film) donné. Les modèles de factorisation matricielles tentent d'approximer les valeurs connues de la matrice des scores par un produit de deux matrices de dimensions inférieurs.\n",
    "\n",
    "## Baseline 1 : Moyenne par utilisateur\n",
    "\n",
    "Ce modèle simple prédit comme note la moyenne des notes données par l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class baselineMeanUsers():\n",
    "    def __init__(self):\n",
    "        self.mean = {}\n",
    "    def fit(self, dataUsers):\n",
    "        self.mean = {}\n",
    "        for u in dataUsers.keys():\n",
    "            self.mean[u] = 0\n",
    "            for i in dataUsers[u].keys():\n",
    "                self.mean[u] = self.mean[u] + dataUsers[u][i]\n",
    "            self.mean[u] = self.mean[u] / len(dataUsers[u])\n",
    "    def predict(self, couplesTest):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.mean[c[0]]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2 : Moyenne par item\n",
    "\n",
    "Ce modèle simple prédit comme note la moyenne des notes données à l'objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class baselineMeanItems():\n",
    "    def __init__(self):            \n",
    "        self.mean = {}\n",
    "    def fit(self, dataItems):\n",
    "        self.mean = {}\n",
    "        for i in dataItems.keys():\n",
    "            self.mean[i] = 0\n",
    "            for u in dataItems[i].keys():\n",
    "                self.mean[i] = self.mean[i] + dataItems[i][u]\n",
    "            self.mean[i] = self.mean[i] / len(dataItems[i])\n",
    "    def predict(self, couplesTest):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.mean[c[1]]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle sans biais\n",
    "\n",
    "On calcule les deux matrices P et Q tel que pour les exemples connus, PQ ~= X, où X est la matrice des scores.\n",
    "Pour prédire, il suffit alors de lire dans la matrice PQ les nouveaux exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class matrixFactorisation():\n",
    "    def __init__(self, k, lambd=0.2, eps=1e-5, maxIter=2000, alternate=0):\n",
    "        self.k = k\n",
    "        self.lambd = lambd\n",
    "        self.eps = eps\n",
    "        self.maxIter = maxIter\n",
    "        self.alternate = alternate #Pour l'optimisation alternée: 0 si non.\n",
    "    def fit(self, dataUsers, dataItems, couples):\n",
    "        self.p = {}\n",
    "        self.q = {}\n",
    "        self.loss = []\n",
    "        #Choix du paramètre a optimisé en cas d'optimisation alternée\n",
    "        optimP = True\n",
    "        optimQ = (self.alternate == 0)\n",
    "        for i in xrange(self.maxIter):\n",
    "            loss = 0\n",
    "            for j in xrange(len(couples)):\n",
    "                #choix d'une entrée aléatoire\n",
    "                r = np.random.randint(len(couples)) \n",
    "                user = couples[r][0]\n",
    "                item = couples[r][1]\n",
    "                # initialisation des nouveaux vecteurs p et q\n",
    "                if not user in self.p:\n",
    "                    self.p[user] = np.random.rand(1,self.k)\n",
    "                if not item in self.q:\n",
    "                    self.q[item] = np.random.rand(self.k,1)\n",
    "                # Descente de gradient\n",
    "                tmp = dataUsers[user][item] - self.p[user].dot(self.q[item])[0][0]\n",
    "                if (optimP):\n",
    "                    self.p[user] = (1 - self.lambd * self.eps) * self.p[user] + self.eps * 2 * tmp * self.q[item].transpose()\n",
    "                if (optimQ):\n",
    "                    self.q[item] = (1 - self.lambd * self.eps) * self.q[item] + self.eps * 2 * tmp * self.p[user].transpose()\n",
    "                loss = loss + tmp*tmp #(Sans le terme de régularisation)\n",
    "            self.loss.append(loss)\n",
    "            # Optimisation alternée\n",
    "            if (self.alternate != 0):\n",
    "                if (i % self.alternate == 0):\n",
    "                    optimP = optimQ\n",
    "                    optimQ = 1 - optimQ\n",
    "                    print i, loss / len(couples)\n",
    "            else:\n",
    "                if (i % 100 == 0):\n",
    "                    print i, loss / len(couples)\n",
    "    def predict(self, couplesTest):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.p[c[0]].dot(self.q[c[1]])[0][0]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle avec biais\n",
    "\n",
    "Ce modèle tiens compte d'un biais propre à chaque utilisateur et un biais propre à chaque film.\n",
    "On cherche donc le scalaire $\\mu$ et les matrices $P$,$Q$,$B_u$,$B_i$ tel que pour les exemples $(u,i)$ connus, $PQ(u,i) + B_u(u) + B_i(i) + \\mu ~= X(u,i)$, où $X$ est la matrice des scores.\n",
    "Pour prédire le score d'un nouvel exemple (u',i'), il suffit de calculer $PQ(u,i) + B_u(u) + B_i(i) + \\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comme matrixFactorisation() avec plus de paramètres\n",
    "class matrixFactorisationBiais():\n",
    "    def __init__(self, k, lambd=0.2, eps=1e-5, maxIter=10000, alternate=0):\n",
    "        self.k = k\n",
    "        self.lambd = lambd\n",
    "        self.eps = eps\n",
    "        self.maxIter = maxIter\n",
    "        self.alternate = alternate\n",
    "    def fit(self, dataUsers, dataItems, couples):\n",
    "        self.p = {}\n",
    "        self.q = {}\n",
    "        self.bu = {}\n",
    "        self.bi = {}\n",
    "        self.mu = np.random.random() * 2 - 1\n",
    "        self.loss = []\n",
    "        optimP = True\n",
    "        optimQ = (self.alternate == 0)\n",
    "        for i in xrange(self.maxIter):\n",
    "            loss = 0\n",
    "            for j in xrange(len(couples)):\n",
    "                r = np.random.randint(len(couples))\n",
    "                user = couples[r][0]\n",
    "                item = couples[r][1]\n",
    "                if not user in self.p:\n",
    "                    self.p[user] = np.random.rand(1,self.k) * 2 - 1\n",
    "                    self.bu[user] = np.random.rand() * 2 - 1\n",
    "                if not item in self.q:\n",
    "                    self.q[item] = np.random.rand(self.k,1) * 2 - 1\n",
    "                    self.bi[item] = np.random.rand() * 2 - 1\n",
    "                tmp = dataUsers[user][item] - (self.mu + self.bi[item] + self.bu[user] + self.p[user].dot(self.q[item])[0][0])\n",
    "                if (optimP):\n",
    "                    self.p[user] = (1 - self.lambd * self.eps) * self.p[user] + self.eps * 2 * tmp * self.q[item].transpose()\n",
    "                    self.bu[user] = (1 - self.lambd * self.eps) * self.bu[user] + self.eps * 2 * tmp\n",
    "                if (optimQ):\n",
    "                    self.q[item] = (1 - self.lambd * self.eps) * self.q[item] + self.eps * 2 * tmp * self.p[user].transpose()\n",
    "                    self.bi[item] = (1 - self.lambd * self.eps) * self.bi[item] + self.eps * 2 * tmp\n",
    "                self.mu = (1 - self.lambd * self.eps) * self.mu + self.eps * 2 * tmp\n",
    "                loss = loss + tmp*tmp\n",
    "            self.loss.append(loss)\n",
    "            if (self.alternate != 0):\n",
    "                if (i % self.alternate == 0):\n",
    "                    optimP = optimQ\n",
    "                    optimQ = 1 - optimQ\n",
    "                    print i, loss / len(couples)\n",
    "            else:\n",
    "                if (i % 100 == 0):\n",
    "                    print i, loss / len(couples)\n",
    "    def predict(self, couplesTest):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.mu + self.bu[c[0]] + self.bi[c[1]] + self.p[c[0]].dot(self.q[c[1]])[0][0]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle avec biais temporel\n",
    "\n",
    "Pour un couple utilisateur-film $(u,i)$, Les biais $B_u(u)$, $B_i(i)$ et $\\mu$ varient dans le temps. On modélise celà par des vecteurs $B_u(u,t)$, $B_i(i,t)$ et $\\mu(t)$ qui nous donnent pour chaque temps la valeur du biais correspondant.\n",
    "Les entrées du modèles sont alors un triplet $(u,i,t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class matrixFactorisationBiaisTemporel():\n",
    "    def __init__(self, k=10, ntimes=5, lambd=0.2, eps=1e-5, maxIter=10000, alternate=0):\n",
    "        self.k = k\n",
    "        self.ntimes = ntimes\n",
    "        self.lambd = lambd\n",
    "        self.eps = eps\n",
    "        self.maxIter = maxIter\n",
    "        self.alternate = alternate\n",
    "    def fit(self, dataUsers, dataItems, couples, times):\n",
    "        self.p = {}\n",
    "        self.q = {}\n",
    "        self.bu = {}\n",
    "        self.bi = {}\n",
    "        self.mu = np.random.rand(self.ntimes) * 2 - 1\n",
    "        self.loss = []\n",
    "        optimP = True\n",
    "        optimQ = (self.alternate == 0)\n",
    "        for i in xrange(self.maxIter):\n",
    "            loss = 0\n",
    "            for j in xrange(len(couples)):\n",
    "                r = np.random.randint(len(couples))\n",
    "                user = couples[r][0]\n",
    "                item = couples[r][1]\n",
    "                time = times[r]\n",
    "                if not user in self.p:\n",
    "                    self.p[user] = np.random.rand(1,self.k) * 2 - 1\n",
    "                    self.bu[user] = np.random.rand(self.ntimes) * 2 - 1\n",
    "                if not item in self.q:\n",
    "                    self.q[item] = np.random.rand(self.k,1) * 2 - 1\n",
    "                    self.bi[item] = np.random.rand(self.ntimes) * 2 - 1\n",
    "                tmp = dataUsers[user][item] - (self.mu[time] + self.bi[item][time] + self.bu[user][time] + self.p[user].dot(self.q[item])[0][0])\n",
    "                if (optimP):\n",
    "                    self.p[user] = (1 - self.lambd * self.eps) * self.p[user] + self.eps * 2 * tmp * self.q[item].transpose()\n",
    "                    self.bu[user][time] = (1 - self.lambd * self.eps) * self.bu[user][time] + self.eps * 2 * tmp\n",
    "                if (optimQ):\n",
    "                    self.q[item] = (1 - self.lambd * self.eps) * self.q[item] + self.eps * 2 * tmp * self.p[user].transpose()\n",
    "                    self.bi[item][time] = (1 - self.lambd * self.eps) * self.bi[item][time] + self.eps * 2 * tmp\n",
    "                self.mu[time] = (1 - self.lambd * self.eps) * self.mu[time] + self.eps * 2 * tmp\n",
    "                loss = loss + tmp*tmp #Sans régularisation\n",
    "            self.loss.append(loss)\n",
    "            if (self.alternate != 0):\n",
    "                if (i % self.alternate == 0):\n",
    "                    optimP = optimQ\n",
    "                    optimQ = 1 - optimQ\n",
    "                    print i, loss / len(couples)\n",
    "            else:\n",
    "                if (i % 100 == 0):\n",
    "                    print i, loss / len(couples)\n",
    "    def predict(self, couplesTest, times):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.mu[times[ind]] + self.bu[c[0]][times[ind]] + self.bi[c[1]][times[ind]] + self.p[c[0]].dot(self.q[c[1]])[0][0]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests les données Movie Lens 100k\n",
    "\n",
    "Les données Movie Lens 100k comprennent 100 000 scores données par 1000 utilisateurs sur 1700 films.\n",
    "\n",
    "## Préparation des données\n",
    "\n",
    "On extrait aléatoirement une portion (20%) des données pour constituer la base de test, et le reste sera utilisé en apprentissage.\n",
    "\n",
    "Comme on ne souhaite ne pas évaluer les objets et les utilisateurs qui n'ont jamais été rencontré en apprentissage, on retire les couples correspondants de l'ensemble de test.\n",
    "\n",
    "Reste ensuite à reconstruire les deux dictionnaires a partir de ces liste de couples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement\n",
    "data, timestamps = loadMovieLens()\n",
    "\n",
    "# Récupérer la représentation en liste de triplets\n",
    "couples = getCouplesUsersItems(data)\n",
    "\n",
    "# La séparer en ensemble d'apprentissage et de test\n",
    "trainCouples, testCouples = splitTrainTest(couples,.20)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble d'apprentissage\n",
    "trainUsers = buildUsersDict(trainCouples)\n",
    "trainItems = buildItemsDict(trainCouples)\n",
    "\n",
    "# Supprimer de l'ensemble de test les éléments inconnus en apprentissage\n",
    "toDel = []\n",
    "for i,c in enumerate(testCouples):\n",
    "    if not c[0] in trainUsers:\n",
    "        toDel.append(i)\n",
    "    elif not c[1] in trainItems:\n",
    "        toDel.append(i)\n",
    "testCouples = np.delete(testCouples, toDel, 0)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble de test\n",
    "testUsers  = buildUsersDict(testCouples)\n",
    "testItems  = buildItemsDict(testCouples)\n",
    "\n",
    "# Récupérer les vecteurs des temps\n",
    "nbins = 5\n",
    "times = getTimeBins(couples, timestamps, nbins)\n",
    "trainTimes = getTimeBins(trainCouples, timestamps, nbins)\n",
    "testTimes = getTimeBins(testCouples, timestamps, nbins)\n",
    "\n",
    "# taille des données\n",
    "#print len(trainUsers), len(testUsers)\n",
    "#print len(trainItems), len(testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "On évalue ici les scores des baselines.\n",
    "Comme les notes sont entre 1 et 5, on peut remarquer qu'en prédisant 3 tout le temps, on a nécessairement une erreur inférieur ou égale à 2.\n",
    "\n",
    "Les deux baselines nous donnent un score un test d'environ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur en test: 1.08839474696\n"
     ]
    }
   ],
   "source": [
    "model1 = baselineMeanUsers()\n",
    "model1.fit(trainUsers)\n",
    "pred = model1.predict(testCouples)\n",
    "print \"erreur en test:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur en test: 1.03859106198\n"
     ]
    }
   ],
   "source": [
    "model2 = baselineMeanItems()\n",
    "model2.fit(trainItems)\n",
    "pred = model2.predict(testCouples)\n",
    "print \"erreur en test:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle sans biais\n",
    "\n",
    "Après apprentissage, la factorisation matricielle donne une erreur moyenne en test de 0.9\n",
    "Il est meilleur que les baselines de 0.1 point.\n",
    "On note que le loss en apprentissage est de 0.84. \n",
    "Il est possible que l'on obtienne de meilleurs score de généralisation en test en augmentant la régularisation mais au vu du score actuel en apprentissage, le gain devrait rester assez faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.829537706\n",
      "100 1.30712001798\n",
      "200 1.07948970774\n",
      "300 0.997287439085\n",
      "400 0.952657605705\n",
      "500 0.917643772482\n",
      "600 0.89365902144\n",
      "700 0.888028692502\n",
      "800 0.869048170694\n",
      "900 0.868529460105\n",
      "1000 0.858853615837\n",
      "1100 0.863836741598\n",
      "1200 0.852722256483\n",
      "1300 0.842192524637\n",
      "1400 0.841983968252\n",
      "1500 0.844243257703\n",
      "1600 0.837896616117\n",
      "1700 0.837744282998\n",
      "1800 0.832008944357\n",
      "1900 0.83920613112\n"
     ]
    }
   ],
   "source": [
    "model3 = matrixFactorisation(10, alternate=0)\n",
    "model3.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model3.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de test: 0.909580818424\n"
     ]
    }
   ],
   "source": [
    "pred = model3.predict(testCouples)\n",
    "print \"Erreur de test:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle avec biais\n",
    "\n",
    "En complexifiant le modèle avec un biais, on obtient à nouveau un score en généralisation de 0.90.\n",
    "Cependant, ici le loss en apprentissage est de 0.71. Il y a une marge importante entre le score en test et celui obtenue en apprentissage. On est vraisemblablement en surapprentissage et il faudrait augmenter le paramètre de la régularisation ou réduire la dimension de la factorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.84879153329\n",
      "100 2.20841845373\n",
      "200 1.78135161261\n",
      "300 1.50665884754\n",
      "400 1.36179780473\n",
      "500 1.27271620043\n",
      "600 1.18725411492\n",
      "700 1.13627534893\n",
      "800 1.08727524392\n",
      "900 1.05073590425\n",
      "1000 1.01654162391\n",
      "1100 1.00560542752\n",
      "1200 0.97646470343\n",
      "1300 0.967124816417\n",
      "1400 0.950623023452\n",
      "1500 0.94110060945\n",
      "1600 0.928282231649\n",
      "1700 0.913576022519\n",
      "1800 0.90368654631\n",
      "1900 0.889919122869\n",
      "2000 0.886632345112\n",
      "2100 0.87637080938\n",
      "2200 0.879517708617\n",
      "2300 0.872767670014\n",
      "2400 0.863631577612\n",
      "2500 0.853651927641\n",
      "2600 0.849095197608\n",
      "2700 0.855620811393\n",
      "2800 0.843541206755\n",
      "2900 0.844477373661\n",
      "3000 0.841569307323\n",
      "3100 0.842851965166\n",
      "3200 0.833419184501\n",
      "3300 0.824305787357\n",
      "3400 0.832219602661\n",
      "3500 0.825525993355\n",
      "3600 0.824484381541\n",
      "3700 0.816336008193\n",
      "3800 0.815159277787\n",
      "3900 0.814879026143\n",
      "4000 0.813612289994\n",
      "4100 0.813574241683\n",
      "4200 0.802972958589\n",
      "4300 0.811059457145\n",
      "4400 0.804668565967\n",
      "4500 0.800480942454\n",
      "4600 0.803039122674\n",
      "4700 0.798049443498\n",
      "4800 0.797483731818\n",
      "4900 0.796329706711\n",
      "5000 0.789174918466\n",
      "5100 0.791177143524\n",
      "5200 0.786608210483\n",
      "5300 0.788704131071\n",
      "5400 0.784770640646\n",
      "5500 0.781415889645\n",
      "5600 0.788106017931\n",
      "5700 0.780995331137\n",
      "5800 0.768241354766\n",
      "5900 0.772388355041\n",
      "6000 0.770515710955\n",
      "6100 0.778211415174\n",
      "6200 0.774700133019\n",
      "6300 0.779695322178\n",
      "6400 0.766843838768\n",
      "6500 0.766804444781\n",
      "6600 0.761505748972\n",
      "6700 0.765190115456\n",
      "6800 0.765347829132\n",
      "6900 0.764181275541\n",
      "7000 0.755660379782\n",
      "7100 0.756277149286\n",
      "7200 0.758423586497\n",
      "7300 0.758824592247\n",
      "7400 0.755913434182\n",
      "7500 0.74682818907\n",
      "7600 0.748383203179\n",
      "7700 0.746113895937\n",
      "7800 0.739821600998\n",
      "7900 0.748992464971\n",
      "8000 0.744560945647\n",
      "8100 0.747488962401\n",
      "8200 0.74483471385\n",
      "8300 0.741946324107\n",
      "8400 0.737835278301\n",
      "8500 0.733317991715\n",
      "8600 0.737173402373\n",
      "8700 0.742342540094\n",
      "8800 0.731533756674\n",
      "8900 0.728225962676\n",
      "9000 0.728983224634\n",
      "9100 0.730584312803\n",
      "9200 0.726048811083\n",
      "9300 0.724669003038\n",
      "9400 0.722043913633\n",
      "9500 0.729239252944\n",
      "9600 0.721099577929\n",
      "9700 0.718941520324\n",
      "9800 0.723550634396\n",
      "9900 0.713612781405\n"
     ]
    }
   ],
   "source": [
    "model4 = matrixFactorisationBiais(10, alternate=0)\n",
    "model4.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model4.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90047122114\n"
     ]
    }
   ],
   "source": [
    "pred = model4.predict(testCouples)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biais Temporel\n",
    "\n",
    "En introduisant un biais temporel, on complexifie à nouveau le modèle. Il est vraisemblable qu'on se retrouve a nouveau en surapprentissage si l'on conserve les même paramètres.\n",
    "\n",
    "### Visualisition des notes en fonction du temps\n",
    "Malheureusement, nous n'avons pas eu le temps de mener des expériences concluantes.\n",
    "Cependant, nous avons visualiser la distribution des scores en fonction du des bins de temps.\n",
    "\n",
    "Sur l'ensemble des utilisateurs et des objets, le biais ne varie que relativement peu dans le temps.\n",
    "Mais ces courbes ne permettent pas de dire l'évolution pour un utilisateur ou pour un objet donné.\n",
    "\n",
    "Chaque courbe représente un bins temporel.\n",
    "En abscisse les notes données.\n",
    "En ordonné: la proportion de la note.\n",
    "![alt text](./ratingsByTime1k.png \"Distribution des scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = np.array(np.array(couples)[:,2], float)\n",
    "plt.figure()\n",
    "for i in xrange(nbins):\n",
    "    histi = np.bincount(np.array(ratings[times==i], int))\n",
    "    plt.plot(1.* histi / histi.sum() , 'o-')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résultats\n",
    "\n",
    "A faire...\n",
    "Nous n'avons pas eu le temps de les mener à bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5 = matrixFactorisationBiaisTemporel(10, alternate=0)\n",
    "model5.fit(trainUsers, trainItems, trainCouples, trainTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model5.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model5.predict(testCouples, testTimes)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiences sur les données Movie Lens 1M\n",
    "\n",
    "Le dataset Movie Lens 1M contient 1 million d'entrées, données par 6000 utiilisateurs sur 4000 films.\n",
    "\n",
    "Une remarque que l'on peut déjà faire est que la matrice des scores est \"moins sparse\" que celle issue de la base 100k.\n",
    "\n",
    "En effet, pour la base 100k on a 100000 notes pour une matrice 1000x1700, soit un remplissage de 17% de la matrice.\n",
    "\n",
    "Pour la base 1M, on a 1 000 000 de notes pour une matrice 6000x4000, soit 24% de remplissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chargement\n",
    "data, timestamps = loadMovieLens1M()\n",
    "\n",
    "# Récupérer la représentation en liste de triplets\n",
    "couples = getCouplesUsersItems(data)\n",
    "\n",
    "# Séparer en ensemble d'apprentissage et de test\n",
    "trainCouples, testCouples = splitTrainTest(couples,.20)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble d'apprentissage\n",
    "trainUsers = buildUsersDict(trainCouples)\n",
    "trainItems = buildItemsDict(trainCouples)\n",
    "\n",
    "# Supprimer de l'ensemble de test les éléments inconnus en apprentissage\n",
    "toDel = []\n",
    "for i,c in enumerate(testCouples):\n",
    "    if not c[0] in trainUsers:\n",
    "        toDel.append(i)\n",
    "    elif not c[1] in trainItems:\n",
    "        toDel.append(i)\n",
    "testCouples = np.delete(testCouples, toDel, 0)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble de test\n",
    "testUsers  = buildUsersDict(testCouples)\n",
    "testItems  = buildItemsDict(testCouples)\n",
    "\n",
    "# Récupérer les vecteurs des temps\n",
    "nbins = 5\n",
    "times = getTimeBins(couples, timestamps, nbins)\n",
    "trainTimes = getTimeBins(trainCouples, timestamps, nbins)\n",
    "testTimes = getTimeBins(testCouples, timestamps, nbins)\n",
    "\n",
    "# taille des données\n",
    "#print len(trainUsers), len(testUsers)\n",
    "#print len(trainItems), len(testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "Si le score de la baseline par utilisateur est similaire comparé au dataset précédent, le score de baseline par objet en revanche est meilleur que sur la base 100k.\n",
    "On est ici à 0.96 d'erreur moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur en test: 1.07459570403\n"
     ]
    }
   ],
   "source": [
    "model6 = baselineMeanUsers()\n",
    "model6.fit(trainUsers)\n",
    "pred = model6.predict(testCouples)\n",
    "print \"erreur en test:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur en test: 0.958876325654\n"
     ]
    }
   ],
   "source": [
    "model7 = baselineMeanItems()\n",
    "model7.fit(trainItems)\n",
    "pred = model7.predict(testCouples)\n",
    "print \"erreur en test:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation Matricielle\n",
    "\n",
    "Avec un score en apprentissage de 0.82 et de généralisation de 0.85, on peut estimer que le paramètre de régularisation choisi (lambda = 0.2) est raisonnable pour ce problème. La factorisation matricielle est aussi meilleure que celle de la base 100k, probablement parceque la matrice d'apprentissage est moins sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.85630500372\n",
      "100 0.989208053185\n",
      "200 0.894268607039\n",
      "300 0.859435186046\n",
      "400 0.843744226415\n",
      "500 0.834776300235\n",
      "600 0.829587454159\n",
      "700 0.826814746078\n",
      "800 0.820488518828\n",
      "900 0.820793261528\n"
     ]
    }
   ],
   "source": [
    "model8 = matrixFactorisation(10, alternate=0, maxIter=1000)\n",
    "model8.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model8.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848198539785\n"
     ]
    }
   ],
   "source": [
    "pred = model8.predict(testCouples)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation Matricielle avec biais\n",
    "\n",
    "Le score obtenue en apprentissage est similaire au modèle précédent, sans biais. Il est possible qu'il faille continuer l'apprentissage pour l'améliorer.\n",
    "\n",
    "Cependant, le score en généralisation est moins bon que celui en apprentissage, ainsi que de celui en généralisation du modèle précedent, plus simple. Le modèle étant plus complexe, il a vraisemblablement commencé à surapprendre les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.47218672534\n",
      "100 1.53995091541\n",
      "200 1.22517433171\n",
      "300 1.09549293316\n",
      "400 1.02389512199\n",
      "500 0.978845337117\n",
      "600 0.94824016809\n",
      "700 0.925148099365\n",
      "800 0.90515288607\n",
      "900 0.894416953748\n",
      "1000 0.88204288694\n",
      "1100 0.871257580652\n",
      "1200 0.864077916237\n",
      "1300 0.858869586043\n",
      "1400 0.852355995456\n",
      "1500 0.847843742846\n",
      "1600 0.841015305358\n",
      "1700 0.836168926899\n",
      "1800 0.835827627137\n",
      "1900 0.832978355848\n"
     ]
    }
   ],
   "source": [
    "model9 = matrixFactorisationBiais(10, alternate=0, maxIter=2000)\n",
    "model9.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880414019261\n"
     ]
    }
   ],
   "source": [
    "pred = model9.predict(testCouples)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model9.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation de Matrices avec biais temporel\n",
    "\n",
    "Encore une fois, les expériences avec biais temporel n'ont pas encore été menées a bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model10 = matrixFactorisationBiaisTemporel(10, alternate=0, maxIter=2000)\n",
    "model10.fit(trainUsers, trainItems, trainCouples, trainTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model10.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model10.predict(testCouples, testTimes)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Dans ce travail, nous avons implémenté des modèles de filtrage collaboratif que nous avons ensuite chercher à évaluer.\n",
    "Si on a observé qu'un modèle simple de factorisation matricielle pouvait obtenir des résultats concluants, meilleurs qu'une baseline naïve, nous n'avons pu démontré d'intérêt pratiques des modèles avec biais.\n",
    "\n",
    "Cependant, nous avons observés que si nos modèles sans biais avaient des scores similaires en apprentissage qu'en généralisation, ce n'était pas le cas de nos modèles avec biais qui obtiennent de bien meilleurs scores en apprentissage qui ne se traduisent pas en test. On peut en déduire que nous n'avons pas déterminé les bons hyperparamètres pour permettre une bonne généralisation, et que vraisemblablement on peut encore augmenter le score en généralisation de nos modèles avec biais.\n",
    "\n",
    "Enfin, nous n'avons eu le temps ni d'évaluer les modèles avec biais temporel, ni l'influence de la dimension de factorisation matricielle, que l'on peut aussi voir comme une forme de régularisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
