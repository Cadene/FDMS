{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME5 - t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "Les fonctions pour charger les bases Movie Lens 100k et Movie Lens 1M.\n",
    "On récupère un dictionnaire pour les scores et un dictionnaire pour les dates.\n",
    "Le première index de ces dictionnaires est l'identifiant de l'utilisateur, et le second les films notés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadMovieLens(path='./data100k'):\n",
    "    # Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/u.item'):\n",
    "        (id,title)=line.split('|')[0:2]\n",
    "        movies[id]=title\n",
    "    # Load data\n",
    "    prefs={} # Un dictionnaire User > Item > Rating\n",
    "    times={} # Un dictionnaire User > Item > Timestamps\n",
    "    for line in open(path+'/u.data'):\n",
    "        (user,movieid,rating,ts)=line.split('\\t')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "        times.setdefault(user,{})\n",
    "        times[user][movies[movieid]]=float(ts)\n",
    "    return prefs, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMovieLens1M(path='./data1m'):\n",
    "    # Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/movies.dat'):\n",
    "        id,title=line.split('::')[0:2]\n",
    "        movies[id]=title\n",
    "    # Load data\n",
    "    prefs={}\n",
    "    times={}\n",
    "    for line in open(path+'/ratings.dat'):\n",
    "        (user,movieid,rating,ts)=line.split('::')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "        times.setdefault(user,{})\n",
    "        times[user][movies[movieid]]=float(ts)\n",
    "    return prefs, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentations des données\n",
    "\n",
    "Les matrices des scores Utilisateurs/Films sont des matrices de grandes dimensions mais sparses.\n",
    "Afin de les manipuler efficacement, on emploiera 3 représentations différentes en même temps:\n",
    "- Le dictionnaire des scores par utilisateurs: User > Item > Value\n",
    "- Le dictionnaire des scores par films: Item > User > Value \n",
    "- La liste des triplets [User, Item, Value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recupère une représentation des données sous la forme triplets [user, item, value] a partir d'un dictionnaire [User > item > value]\n",
    "def getCouplesUsersItems(data):\n",
    "    couples = []\n",
    "    for u in data.keys():\n",
    "        for i in data[u].keys():\n",
    "            couples.append([u,i,data[u][i]])\n",
    "    return couples\n",
    "\n",
    "# Construit le dictionnaire des utilisateurs a partir des triplets [user, item, note]\n",
    "def buildUsersDict(couples):\n",
    "    dicUsers = {}\n",
    "    for c in couples:\n",
    "        if not c[0] in dicUsers.keys():\n",
    "            dicUsers[c[0]] = {}\n",
    "        dicUsers[c[0]][c[1]] = float(c[2])\n",
    "    return dicUsers\n",
    "\n",
    "# Construit le dictionnaire des objets a partir des triplets [user, item, note]\n",
    "def buildItemsDict(couples):\n",
    "    dicItems = {}\n",
    "    for c in couples:\n",
    "        if not c[1] in dicItems:\n",
    "            dicItems[c[1]] = {}\n",
    "        dicItems[c[1]][c[0]] = float(c[2])\n",
    "    return dicItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données de temps\n",
    "\n",
    "Afin d'exploiter les données temporelles, on discrétise le temps en bins et on construit un vecteur qui a chaque triplets [user, item, value] associe le bins temporel correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTimeBins(couples, timedic, nbins):\n",
    "    timestamps = np.zeros(len(couples))\n",
    "    for i,c in enumerate(couples):\n",
    "        timestamps[i] = timedic[c[0]][c[1]]\n",
    "    time_bins = np.linspace(np.min(timestamps), np.max(timestamps), nbins+1)\n",
    "    times = np.zeros(len(couples),int)\n",
    "    for i in xrange(1,len(time_bins)):\n",
    "        times = times + (timestamps > time_bins[i])\n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données en Train / Test\n",
    "\n",
    "Pour pouvoir séparer les données en ensembles de Train et de Test, on utilisera la liste des triplets [User, Item, Scores]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split l'ensemble des triplets [user, item, note] en testProp% données de test et (1 - testProp) données de train\n",
    "def splitTrainTest(couples,testProp):\n",
    "    perm = np.random.permutation(couples)\n",
    "    splitIndex = int(testProp * len(couples))\n",
    "    return perm[splitIndex:], perm[:splitIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles\n",
    "\n",
    "On implémente ici les différents modèles. Les baselines prédisent simplement la note moyenne pour un utilisateur (ou pour un film) donné. Les modèles de factorisation matricielles tentent d'approximer les valeurs connues de la matrice des scores par un produit de deux matrices de dimensions inférieurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle sans biais\n",
    "\n",
    "On calcule les deux matrices P et Q tel que pour les exemples connus, PQ ~= X, où X est la matrice des scores.\n",
    "Pour prédire, il suffit alors de lire dans la matrice PQ les nouveaux exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class matrixFactorisation():\n",
    "    def __init__(self, k, lambd=0.2, eps=1e-5, maxIter=2000, alternate=0):\n",
    "        self.k = k\n",
    "        self.lambd = lambd\n",
    "        self.eps = eps\n",
    "        self.maxIter = maxIter\n",
    "        self.alternate = alternate #Pour l'optimisation alternée: 0 si non.\n",
    "    def fit(self, dataUsers, dataItems, couples):\n",
    "        self.p = {}\n",
    "        self.q = {}\n",
    "        self.loss = []\n",
    "        #Choix du paramètre a optimisé en cas d'optimisation alternée\n",
    "        optimP = True\n",
    "        optimQ = (self.alternate == 0)\n",
    "        for i in xrange(self.maxIter):\n",
    "            loss = 0\n",
    "            for j in xrange(len(couples)):\n",
    "                #choix d'une entrée aléatoire\n",
    "                r = np.random.randint(len(couples)) \n",
    "                user = couples[r][0]\n",
    "                item = couples[r][1]\n",
    "                # initialisation des nouveaux vecteurs p et q\n",
    "                if not user in self.p:\n",
    "                    self.p[user] = np.random.rand(1,self.k)\n",
    "                if not item in self.q:\n",
    "                    self.q[item] = np.random.rand(self.k,1)\n",
    "                # Descente de gradient\n",
    "                tmp = dataUsers[user][item] - self.p[user].dot(self.q[item])[0][0]\n",
    "                if (optimP):\n",
    "                    self.p[user] = (1 - self.lambd * self.eps) * self.p[user] + self.eps * 2 * tmp * self.q[item].transpose()\n",
    "                if (optimQ):\n",
    "                    self.q[item] = (1 - self.lambd * self.eps) * self.q[item] + self.eps * 2 * tmp * self.p[user].transpose()\n",
    "                loss = loss + tmp*tmp #(Sans le terme de régularisation)\n",
    "            self.loss.append(loss)\n",
    "            # Optimisation alternée\n",
    "            if (self.alternate != 0):\n",
    "                if (i % self.alternate == 0):\n",
    "                    optimP = optimQ\n",
    "                    optimQ = 1 - optimQ\n",
    "                    print i, loss / len(couples)\n",
    "            else:\n",
    "                if (i % 100 == 0):\n",
    "                    print i, loss / len(couples)\n",
    "    def predict(self, couplesTest):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.p[c[0]].dot(self.q[c[1]])[0][0]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests les données Movie Lens 100k\n",
    "\n",
    "Les données Movie Lens 100k comprennent 100 000 scores données par 1000 utilisateurs sur 1700 films.\n",
    "\n",
    "## Préparation des données\n",
    "\n",
    "On extrait aléatoirement une portion (20%) des données pour constituer la base de test, et le reste sera utilisé en apprentissage.\n",
    "\n",
    "Comme on ne souhaite ne pas évaluer les objets et les utilisateurs qui n'ont jamais été rencontré en apprentissage, on retire les couples correspondants de l'ensemble de test.\n",
    "\n",
    "Reste ensuite à reconstruire les deux dictionnaires a partir de ces liste de couples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement\n",
    "data, timestamps = loadMovieLens()\n",
    "\n",
    "# Récupérer la représentation en liste de triplets\n",
    "couples = getCouplesUsersItems(data)\n",
    "\n",
    "# La séparer en ensemble d'apprentissage et de test\n",
    "trainCouples, testCouples = splitTrainTest(couples,.20)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble d'apprentissage\n",
    "trainUsers = buildUsersDict(trainCouples)\n",
    "trainItems = buildItemsDict(trainCouples)\n",
    "\n",
    "# Supprimer de l'ensemble de test les éléments inconnus en apprentissage\n",
    "toDel = []\n",
    "for i,c in enumerate(testCouples):\n",
    "    if not c[0] in trainUsers:\n",
    "        toDel.append(i)\n",
    "    elif not c[1] in trainItems:\n",
    "        toDel.append(i)\n",
    "testCouples = np.delete(testCouples, toDel, 0)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble de test\n",
    "testUsers  = buildUsersDict(testCouples)\n",
    "testItems  = buildItemsDict(testCouples)\n",
    "\n",
    "# taille des données\n",
    "#print len(trainUsers), len(testUsers)\n",
    "#print len(trainItems), len(testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle sans biais\n",
    "\n",
    "Après apprentissage, la factorisation matricielle donne une erreur moyenne en test de 0.9\n",
    "Il est meilleur que les baselines de 0.1 point.\n",
    "On note que le loss en apprentissage est de 0.84. \n",
    "Il est possible que l'on obtienne de meilleurs score de généralisation en test en augmentant la régularisation mais au vu du score actuel en apprentissage, le gain devrait rester assez faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.71825447884\n",
      "100 1.30461910265\n",
      "200 1.07198937601\n",
      "300 0.988878955293\n",
      "400 0.947013141787\n",
      "500 0.930374148402\n",
      "600 0.901617765719\n",
      "700 0.890678619669\n",
      "800 0.88196674407\n",
      "900 0.867942896422\n",
      "1000 0.863288863963\n",
      "1100 0.86357578657\n",
      "1200 0.854815106275\n",
      "1300 0.847335834343\n",
      "1400 0.847021165112\n",
      "1500 0.846547840724\n",
      "1600 0.854972726777\n",
      "1700 0.845742778706\n",
      "1800 0.849065445756\n",
      "1900 0.831846368653\n"
     ]
    }
   ],
   "source": [
    "model = matrixFactorisation(10, alternate=0)\n",
    "model.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pk.dump(model,  open(\"model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur de test: 0.909580818424\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(testCouples)\n",
    "print \"Erreur de test:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiences sur les données Movie Lens 1M\n",
    "\n",
    "Le dataset Movie Lens 1M contient 1 million d'entrées, données par 6000 utiilisateurs sur 4000 films.\n",
    "\n",
    "Une remarque que l'on peut déjà faire est que la matrice des scores est \"moins sparse\" que celle issue de la base 100k.\n",
    "\n",
    "En effet, pour la base 100k on a 100000 notes pour une matrice 1000x1700, soit un remplissage de 17% de la matrice.\n",
    "\n",
    "Pour la base 1M, on a 1 000 000 de notes pour une matrice 6000x4000, soit 24% de remplissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chargement\n",
    "data, timestamps = loadMovieLens1M()\n",
    "\n",
    "# Récupérer la représentation en liste de triplets\n",
    "couples = getCouplesUsersItems(data)\n",
    "\n",
    "# Séparer en ensemble d'apprentissage et de test\n",
    "trainCouples, testCouples = splitTrainTest(couples,.20)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble d'apprentissage\n",
    "trainUsers = buildUsersDict(trainCouples)\n",
    "trainItems = buildItemsDict(trainCouples)\n",
    "\n",
    "# Supprimer de l'ensemble de test les éléments inconnus en apprentissage\n",
    "toDel = []\n",
    "for i,c in enumerate(testCouples):\n",
    "    if not c[0] in trainUsers:\n",
    "        toDel.append(i)\n",
    "    elif not c[1] in trainItems:\n",
    "        toDel.append(i)\n",
    "testCouples = np.delete(testCouples, toDel, 0)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble de test\n",
    "testUsers  = buildUsersDict(testCouples)\n",
    "testItems  = buildItemsDict(testCouples)\n",
    "\n",
    "# taille des données\n",
    "#print len(trainUsers), len(testUsers)\n",
    "#print len(trainItems), len(testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation Matricielle\n",
    "\n",
    "Avec un score en apprentissage de 0.82 et de généralisation de 0.85, on peut estimer que le paramètre de régularisation choisi (lambda = 0.2) est raisonnable pour ce problème. La factorisation matricielle est aussi meilleure que celle de la base 100k, probablement parceque la matrice d'apprentissage est moins sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.85630500372\n",
      "100 0.989208053185\n",
      "200 0.894268607039\n",
      "300 0.859435186046\n",
      "400 0.843744226415\n",
      "500 0.834776300235\n",
      "600 0.829587454159\n",
      "700 0.826814746078\n",
      "800 0.820488518828\n",
      "900 0.820793261528\n"
     ]
    }
   ],
   "source": [
    "model8 = matrixFactorisation(10, alternate=0, maxIter=1000)\n",
    "model8.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model8.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848198539785\n"
     ]
    }
   ],
   "source": [
    "pred = model8.predict(testCouples)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Dans ce travail, nous avons implémenté des modèles de filtrage collaboratif que nous avons ensuite chercher à évaluer.\n",
    "Si on a observé qu'un modèle simple de factorisation matricielle pouvait obtenir des résultats concluants, meilleurs qu'une baseline naïve, nous n'avons pu démontré d'intérêt pratiques des modèles avec biais.\n",
    "\n",
    "Cependant, nous avons observés que si nos modèles sans biais avaient des scores similaires en apprentissage qu'en généralisation, ce n'était pas le cas de nos modèles avec biais qui obtiennent de bien meilleurs scores en apprentissage qui ne se traduisent pas en test. On peut en déduire que nous n'avons pas déterminé les bons hyperparamètres pour permettre une bonne généralisation, et que vraisemblablement on peut encore augmenter le score en généralisation de nos modèles avec biais.\n",
    "\n",
    "Enfin, nous n'avons eu le temps ni d'évaluer les modèles avec biais temporel, ni l'influence de la dimension de factorisation matricielle, que l'on peut aussi voir comme une forme de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [1 3 2]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3],[4,5,6],[1,3,2],[4,5,6]])\n",
    "print A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 27  2 27]\n",
      " [27  0 29  0]\n",
      " [ 2 29  0 29]\n",
      " [27  0 29  0]]\n"
     ]
    }
   ],
   "source": [
    "norm = np.sum(A**2,1)\n",
    "normi = np.reshape(norm, (1, np.shape(norm)[0]))\n",
    "normi = np.repeat(normi, np.shape(A)[0], 0)\n",
    "distance = normi.T + normi - 2 * A.dot(A.T)\n",
    "print distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.011109    0.77880078  0.06720551]\n",
      " [ 0.00117088  1.          0.0266491   1.        ]\n",
      " [ 0.60653066  0.00795994  1.          0.05502322]\n",
      " [ 0.00117088  1.          0.0266491   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "p = np.exp(- distance / (2 * np.array([2.,3.,4.,5.]))) #sigma²\n",
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60887242,  1.01906894,  0.83209898,  1.12222873])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p - np.eye(np.shape(p)[0]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.64238019  0.01090112  0.93594729  0.05988575]\n",
      " [ 0.00192303  0.98128788  0.03202636  0.89108394]\n",
      " [ 0.99615394  0.007811    1.20178011  0.04903031]\n",
      " [ 0.00192303  0.98128788  0.03202636  0.89108394]]\n"
     ]
    }
   ],
   "source": [
    "ps=p / np.sum(p - np.eye(np.shape(p)[0]),0)\n",
    "print ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41059505,  0.00160302,  0.24151265,  0.0077261 ],\n",
       "       [ 0.00160302,  0.24532197,  0.00497967,  0.23404648],\n",
       "       [ 0.24151265,  0.00497967,  0.30044503,  0.01013208],\n",
       "       [ 0.0077261 ,  0.23404648,  0.01013208,  0.22277098]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ps + ps.T) / (2*np.shape(ps)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tSNE():\n",
    "    def __init__(self, perp, nIter, lr, moment, dim=2):\n",
    "        self.perp = perp # entre 5 et 50\n",
    "        self.nIter = nIter\n",
    "        self.lr = lr\n",
    "        self.moment = moment\n",
    "        self.dim = dim\n",
    "    def fit(self, data):\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #initialiser y et q(j|i)\n",
    "        self.embedding = np.random.randn(np.shape(data)[0], self.dim) * 1e-4\n",
    "        self.qjsi = np.zeros((self.dim,self.dim))\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #initialisation des sigmas et des p(j|i)\n",
    "        self.pjsi = np.zeros((np.shape(data)[0],np.shape(data)[0]))\n",
    "        self.sigmai = np.zeros(np.shape(data)[0]) #il faut calculer les sigma²\n",
    "        self.perpi = np.zeros(np.shape(data)[0])\n",
    "        #initialisation bornes\n",
    "        normx = np.sum((data**2),1)\n",
    "        repnormx = np.reshape(normx, (1, np.shape(normx)[0]))\n",
    "        distancex = repnormx + repnormx.T - 2 * data.dot(data.T)\n",
    "        self.sigmaisup = np.ones(np.shape(data)[0]) * distancex.max()\n",
    "        self.sigmaiinf = np.zeros(np.shape(data)[0])\n",
    "        print(self.sigmaisup.shape)\n",
    "        print(self.sigmaiinf.shape)\n",
    "        self.sigmai = (self.sigmaisup + self.sigmaiinf) / 2.\n",
    "        stop = 0\n",
    "        while stop < 10:\n",
    "            # Matrice des distances ||xi - xj||²\n",
    "            normx = np.sum((data**2),1)\n",
    "            repnormx = np.reshape(normx, (1, np.shape(normx)[0]))\n",
    "            distancex = repnormx + repnormx.T - 2 * data.dot(data.T)\n",
    "            # p(j|i) #en ligne les j, en colonne les i\n",
    "            self.pjsi = np.exp( -distancex / (2 * (self.sigmai**2).reshape((1, self.sigmai.shape[0])) )) \n",
    "            self.pjsi = self.pjsi / np.sum(self.pjsi - np.eye(self.pjsi.shape[0]), 0)\n",
    "            self.perpi = np.power(2, -np.sum(self.pjsi * np.log2(self.pjsi), 1))\n",
    "            print(self.sigmai)\n",
    "            print(self.perpi)\n",
    "            \n",
    "            self.sigmai = self.sigmaisup\n",
    "            self.pjsi = np.exp( -distancex / (2 * (self.sigmai**2).reshape((1, self.sigmai.shape[0])) )) \n",
    "            self.pjsi = self.pjsi / np.sum(self.pjsi - np.eye(self.pjsi.shape[0]),0)\n",
    "            self.perpi = np.power(2, -np.sum(self.pjsi * np.log2(self.pjsi), 1))\n",
    "            print(self.sigmai)\n",
    "            print(self.perpi)\n",
    "            break\n",
    "            \n",
    "            self.difPerp = self.perpi - self.perp\n",
    "            if np.sum( (self.difPerp * np.sign(self.difPerp)) < 1e-2 ) > 0:\n",
    "                break\n",
    "            else:\n",
    "                self.sigmaisup[self.difPerp > 0] = self.sigmai[self.difPerp > 0]\n",
    "                self.sigmaiinf[self.difPerp < 0] = self.sigmai[self.difPerp < 0]\n",
    "                self.sigmai = (self.sigmaisup + self.sigmaiinf) / 2\n",
    "                stop += 1\n",
    "                print('-------------')\n",
    "                print('sigmaisup') \n",
    "                print(self.sigmaisup)\n",
    "                print('sigmaiinf') \n",
    "                print(self.sigmaiinf)\n",
    "                print('sigmai') \n",
    "                print(self.sigmai)\n",
    "                print('perpi') \n",
    "                print(self.perpi)\n",
    "                print('difPerp')\n",
    "                print(self.difPerp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n",
      "[ 0.88673163  0.88673163  0.88673163  0.88673163  0.88673163]\n",
      "[ 4.91108741  5.27588474  5.41632133  6.01848571  5.61279523]\n",
      "[ 1.77346327  1.77346327  1.77346327  1.77346327  1.77346327]\n",
      "[ 5.65988018  5.70946989  5.73461723  5.83950589  5.76283878]\n"
     ]
    }
   ],
   "source": [
    "model = tSNE(perp=5, nIter=100, lr=1e-3, moment=0.4, dim=2)\n",
    "data = np.random.rand(5,10)\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-5db118d56fc2>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-5db118d56fc2>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    self.pjsi = np.exp(-distance / 2 self.sigma )\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class tSNE():\n",
    "    def __init__(perp, nIter, lr, moment, dim=2):\n",
    "        self.perp = perp # entre 5 et 50\n",
    "        self.nIter = nIter\n",
    "        self.lr = lr\n",
    "        self.moment = moment\n",
    "        self.dim = dim\n",
    "    def fit(data):\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #initialiser y et q(j|i)\n",
    "        self.embedding = np.random.randn(np.shape(data)[0], self.dim) * 1e-4\n",
    "        self.qjsi = np.zeros((self.dim,self.dim))\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #initialisation des sigmas et des p(j|i)\n",
    "        self.pjsi = np.zeros((np.shape(data)[0],np.shape(data)[0]))\n",
    "        self.sigmai = np.zeros(np.shape(data)) #il faut calculer les sigma²\n",
    "        self.perpi = np.zeros(np.shape(data))\n",
    "        #initialisation bornes\n",
    "        normx = np.sum((data**2),1)\n",
    "        repnormx = np.reshape(normx, (1, np.shape(normx)[0]))\n",
    "        distancex = repnormx + repnormx.T - 2 * data.dot(data.T)\n",
    "        self.sigmaisup = np.ones(np.shape(data)) * distancex.max()\n",
    "        self.sigmaiinf = np.zeros(np.shape(data)\n",
    "        self.sigmai = sigmaisup + sigmaiinf / 2\n",
    "        while True:\n",
    "            # Matrice des distances ||xi - xj||²\n",
    "            normx = np.sum((data**2),1)\n",
    "            repnormx = np.reshape(normx, (1, np.shape(normx)[0]))\n",
    "            distancex = repnormx + repnormx.T - 2 * data.dot(data.T)\n",
    "            # p(j|i) #en ligne les j, en colonne les i\n",
    "            self.pjsi = np.exp(-distancex / 2 * self.sigmai ) \n",
    "            self.pjsi = self.pjsi / np.sum(self.pjsi - np.eye(np.shape(self.pjsi)[0],0))\n",
    "            self.perpi = np.power(2, -1 * np.sum(self.pjsi * np.log2(self.pjsi), 0))\n",
    "            difPerp = self.perpi - self.perp\n",
    "            if np.sum( (difPerp * np.sign(difPerp)) < 1e-2 ) == 0:\n",
    "                break\n",
    "            else:\n",
    "                self.sigmaisup[difPerp < 0] = self.sigmai\n",
    "                self.sigmaiinf[difPerp > 0] = self.sigmai\n",
    "                self.sigmai = sigamisup + sigmaiinf / 2\n",
    "        \n",
    "        # p(ij)\n",
    "        self.pij = np(self.pjsi + self.pjsi.T) / (2*np.shape(self.pjsi)[0])\n",
    "                                  \n",
    "        # Descente de Gradient\n",
    "        for t in xrange(self.nIter):\n",
    "            # Matrice des distances ||yi - yj||²\n",
    "            normy = np.sum((self.embedding**2),1)\n",
    "            repnormy = np.reshape(normy, (1, np.shape(normy)[0]))\n",
    "            distancey = repnormy + repnormy.T - 2 * self.embedding.dot(self.embedding.T)\n",
    "            # q(ij)\n",
    "            self.qij = 1 + distancey\n",
    "            self.qij = np.sum(self.qij - np.eye(np.shape(self.qij)[0],0)) / self.qij\n",
    "            # Gradient\n",
    "            tmpgrad = 4 * ((self.pij - self.qij) / (1 + distancey))\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [1, 3, 2],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [-3, -3, -3],\n",
       "       [ 0, -1,  1],\n",
       "       [-3, -3, -3]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0] - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6, -7, -5])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(A[0] - A, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [4 1]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[0,1],[1,2],[3,4],[4,1]])\n",
    "print B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = np.zeros(( ,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
