{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "Les fonctions pour charger les bases Movie Lens 100k et Movie Lens 1M.\n",
    "On récupère un dictionnaire pour les scores et un dictionnaire pour les dates.\n",
    "Le première index de ces dictionnaires est l'identifiant de l'utilisateur, et le second les films notés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadMovieLens(path='./data100k'):\n",
    "    # Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/u.item'):\n",
    "        (id,title)=line.split('|')[0:2]\n",
    "        movies[id]=title\n",
    "    # Load data\n",
    "    prefs={} # Un dictionnaire User > Item > Rating\n",
    "    times={} # Un dictionnaire User > Item > Timestamps\n",
    "    for line in open(path+'/u.data'):\n",
    "        (user,movieid,rating,ts)=line.split('\\t')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "        times.setdefault(user,{})\n",
    "        times[user][movies[movieid]]=float(ts)\n",
    "    return prefs, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMovieLens1M(path='./data1m'):\n",
    "    # Get movie titles\n",
    "    movies={}\n",
    "    for line in open(path+'/movies.dat'):\n",
    "        id,title=line.split('::')[0:2]\n",
    "        movies[id]=title\n",
    "    # Load data\n",
    "    prefs={}\n",
    "    times={}\n",
    "    for line in open(path+'/ratings.dat'):\n",
    "        (user,movieid,rating,ts)=line.split('::')\n",
    "        prefs.setdefault(user,{})\n",
    "        prefs[user][movies[movieid]]=float(rating)\n",
    "        times.setdefault(user,{})\n",
    "        times[user][movies[movieid]]=float(ts)\n",
    "    return prefs, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentations des données\n",
    "\n",
    "Les matrices des scores Utilisateurs/Films sont des matrices de grandes dimensions mais sparses.\n",
    "Afin de les manipuler efficacement, on emploiera 3 représentations différentes en même temps:\n",
    "- Le dictionnaire des scores par utilisateurs: User > Item > Value\n",
    "- Le dictionnaire des scores par films: Item > User > Value \n",
    "- La liste des triplets [User, Item, Value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recupère une représentation des données sous la forme triplets [user, item, value] a partir d'un dictionnaire [User > item > value]\n",
    "def getCouplesUsersItems(data):\n",
    "    couples = []\n",
    "    for u in data.keys():\n",
    "        for i in data[u].keys():\n",
    "            couples.append([u,i,data[u][i]])\n",
    "    return couples\n",
    "\n",
    "# Construit le dictionnaire des utilisateurs a partir des triplets [user, item, note]\n",
    "def buildUsersDict(couples):\n",
    "    dicUsers = {}\n",
    "    for c in couples:\n",
    "        if not c[0] in dicUsers.keys():\n",
    "            dicUsers[c[0]] = {}\n",
    "        dicUsers[c[0]][c[1]] = float(c[2])\n",
    "    return dicUsers\n",
    "\n",
    "# Construit le dictionnaire des objets a partir des triplets [user, item, note]\n",
    "def buildItemsDict(couples):\n",
    "    dicItems = {}\n",
    "    for c in couples:\n",
    "        if not c[1] in dicItems:\n",
    "            dicItems[c[1]] = {}\n",
    "        dicItems[c[1]][c[0]] = float(c[2])\n",
    "    return dicItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation des données en Train / Test\n",
    "\n",
    "Pour pouvoir séparer les données en ensembles de Train et de Test, on utilisera la liste des triplets [User, Item, Scores]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split l'ensemble des triplets [user, item, note] en testProp% données de test et (1 - testProp) données de train\n",
    "def splitTrainTest(couples,testProp):\n",
    "    perm = np.random.permutation(couples)\n",
    "    splitIndex = int(testProp * len(couples))\n",
    "    return perm[splitIndex:], perm[:splitIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles\n",
    "\n",
    "On implémente ici les différents modèles. Les baselines prédisent simplement la note moyenne pour un utilisateur (ou pour un film) donné. Les modèles de factorisation matricielles tentent d'approximer les valeurs connues de la matrice des scores par un produit de deux matrices de dimensions inférieurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle sans biais\n",
    "\n",
    "On calcule les deux matrices P et Q tel que pour les exemples connus, PQ ~= X, où X est la matrice des scores.\n",
    "Pour prédire, il suffit alors de lire dans la matrice PQ les nouveaux exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class matrixFactorisation():\n",
    "    def __init__(self, k, lambd=0.2, eps=1e-5, maxIter=2000, alternate=0):\n",
    "        self.k = k\n",
    "        self.lambd = lambd\n",
    "        self.eps = eps\n",
    "        self.maxIter = maxIter\n",
    "        self.alternate = alternate #Pour l'optimisation alternée: 0 si non.\n",
    "    def fit(self, dataUsers, dataItems, couples):\n",
    "        self.p = {}\n",
    "        self.q = {}\n",
    "        self.loss = []\n",
    "        #Choix du paramètre a optimisé en cas d'optimisation alternée\n",
    "        optimP = True\n",
    "        optimQ = (self.alternate == 0)\n",
    "        for i in xrange(self.maxIter):\n",
    "            loss = 0\n",
    "            for j in xrange(len(couples)):\n",
    "                #choix d'une entrée aléatoire\n",
    "                r = np.random.randint(len(couples)) \n",
    "                user = couples[r][0]\n",
    "                item = couples[r][1]\n",
    "                # initialisation des nouveaux vecteurs p et q\n",
    "                if not user in self.p:\n",
    "                    self.p[user] = np.random.rand(1,self.k)\n",
    "                if not item in self.q:\n",
    "                    self.q[item] = np.random.rand(self.k,1)\n",
    "                # Descente de gradient\n",
    "                tmp = dataUsers[user][item] - self.p[user].dot(self.q[item])[0][0]\n",
    "                if (optimP):\n",
    "                    self.p[user] = (1 - self.lambd * self.eps) * self.p[user] + self.eps * 2 * tmp * self.q[item].transpose()\n",
    "                if (optimQ):\n",
    "                    self.q[item] = (1 - self.lambd * self.eps) * self.q[item] + self.eps * 2 * tmp * self.p[user].transpose()\n",
    "                loss = loss + tmp*tmp #(Sans le terme de régularisation)\n",
    "            self.loss.append(loss)\n",
    "            # Optimisation alternée\n",
    "            if (self.alternate != 0):\n",
    "                if (i % self.alternate == 0):\n",
    "                    optimP = optimQ\n",
    "                    optimQ = 1 - optimQ\n",
    "                    print i, loss / len(couples)\n",
    "            else:\n",
    "                if (i % 100 == 0):\n",
    "                    print i, loss / len(couples)\n",
    "    def predict(self, couplesTest):\n",
    "        pred = np.zeros(len(couplesTest))\n",
    "        for ind,c in enumerate(couplesTest):\n",
    "            pred[ind] = self.p[c[0]].dot(self.q[c[1]])[0][0]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests les données Movie Lens 100k\n",
    "\n",
    "Les données Movie Lens 100k comprennent 100 000 scores données par 1000 utilisateurs sur 1700 films.\n",
    "\n",
    "## Préparation des données\n",
    "\n",
    "On extrait aléatoirement une portion (20%) des données pour constituer la base de test, et le reste sera utilisé en apprentissage.\n",
    "\n",
    "Comme on ne souhaite ne pas évaluer les objets et les utilisateurs qui n'ont jamais été rencontré en apprentissage, on retire les couples correspondants de l'ensemble de test.\n",
    "\n",
    "Reste ensuite à reconstruire les deux dictionnaires a partir de ces liste de couples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement\n",
    "data, timestamps = loadMovieLens()\n",
    "\n",
    "# Récupérer la représentation en liste de triplets\n",
    "couples = getCouplesUsersItems(data)\n",
    "\n",
    "# La séparer en ensemble d'apprentissage et de test\n",
    "trainCouples, testCouples = splitTrainTest(couples,.20)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble d'apprentissage\n",
    "trainUsers = buildUsersDict(trainCouples)\n",
    "trainItems = buildItemsDict(trainCouples)\n",
    "\n",
    "# Supprimer de l'ensemble de test les éléments inconnus en apprentissage\n",
    "toDel = []\n",
    "for i,c in enumerate(testCouples):\n",
    "    if not c[0] in trainUsers:\n",
    "        toDel.append(i)\n",
    "    elif not c[1] in trainItems:\n",
    "        toDel.append(i)\n",
    "testCouples = np.delete(testCouples, toDel, 0)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble de test\n",
    "testUsers  = buildUsersDict(testCouples)\n",
    "testItems  = buildItemsDict(testCouples)\n",
    "\n",
    "# taille des données\n",
    "#print len(trainUsers), len(testUsers)\n",
    "#print len(trainItems), len(testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation matricielle sans biais\n",
    "\n",
    "Après apprentissage, la factorisation matricielle donne une erreur moyenne en test de 0.9\n",
    "Il est meilleur que les baselines de 0.1 point.\n",
    "On note que le loss en apprentissage est de 0.84. \n",
    "Il est possible que l'on obtienne de meilleurs score de généralisation en test en augmentant la régularisation mais au vu du score actuel en apprentissage, le gain devrait rester assez faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.2 -- k = 10\n",
      "0 2.93910030325\n",
      "100 1.28607270087\n",
      "200 1.05507982306\n",
      "300 0.979081544994\n",
      "400 0.936986520146\n",
      "500 0.905293793796\n",
      "600 0.893963796817\n",
      "700 0.877659580173\n",
      "800 0.875634307362\n",
      "900 0.86798563661\n",
      "\n",
      "lambda = 0.2 -- k = 20\n",
      "0 4.27845073048\n",
      "100 1.3493917393\n",
      "200 1.12959401963\n",
      "300 1.04154014588\n",
      "400 0.987881684402\n",
      "500"
     ]
    }
   ],
   "source": [
    "nIter = 1000\n",
    "\n",
    "print 'lambda = 0.2 -- k = 10'\n",
    "model12 = matrixFactorisation(10, lambd=0.2, maxIter=nIter, alternate=0)\n",
    "model12.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.2 -- k = 20'\n",
    "model22 = matrixFactorisation(20, lambd=0.2, maxIter=nIter, alternate=0)\n",
    "model22.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.2 -- k = 30'\n",
    "model32 = matrixFactorisation(30, lambd=0.2, maxIter=nIter, alternate=0)\n",
    "model32.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.3 -- k = 10'\n",
    "model13 = matrixFactorisation(10, lambd=0.3, maxIter=nIter, alternate=0)\n",
    "model13.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.3 -- k = 20'\n",
    "model23 = matrixFactorisation(20, lambd=0.3, maxIter=nIter, alternate=0)\n",
    "model23.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.3 -- k = 30'\n",
    "model33 = matrixFactorisation(30, lambd=0.3, maxIter=nIter, alternate=0)\n",
    "model33.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.5 -- k = 10'\n",
    "model15 = matrixFactorisation(10, lambd=0.5, maxIter=nIter, alternate=0)\n",
    "model15.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.5 -- k = 20'\n",
    "model25 = matrixFactorisation(20, lambd=0.5, maxIter=nIter, alternate=0)\n",
    "model25.fit(trainUsers, trainItems, trainCouples)\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.5 -- k = 30'\n",
    "model35 = matrixFactorisation(30, lambd=0.5, maxIter=nIter, alternate=0)\n",
    "model35.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model3.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.2 -- k = 10\n",
      "Erreur de validation: 2.57357748202\n",
      "lambda = 0.2 -- k = 20\n",
      "Erreur de validation: 3.05853941044\n",
      "lambda = 0.2 -- k = 30\n",
      "Erreur de validation: 9.66017964221\n",
      "lambda = 0.3 -- k = 10\n",
      "Erreur de validation: 2.46197258496\n",
      "lambda = 0.3 -- k = 20\n",
      "Erreur de validation: 3.06013186405\n",
      "lambda = 0.3 -- k = 30\n",
      "Erreur de validation: 9.41218630305\n",
      "lambda = 0.5 -- k = 10\n",
      "Erreur de validation: 2.48612860899\n",
      "lambda = 0.5 -- k = 20\n",
      "Erreur de validation: 3.12848894695\n",
      "lambda = 0.5 -- k = 30\n",
      "Erreur de validation: 9.29991784042\n"
     ]
    }
   ],
   "source": [
    "print 'lambda = 0.2 -- k = 10'\n",
    "pred = model12.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.2 -- k = 20'\n",
    "pred = model22.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.2 -- k = 30'\n",
    "pred = model32.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.3 -- k = 10'\n",
    "pred = model13.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.3 -- k = 20'\n",
    "pred = model23.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.3 -- k = 30'\n",
    "pred = model33.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.5 -- k = 10'\n",
    "pred = model15.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.5 -- k = 20'\n",
    "pred = model25.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()\n",
    "\n",
    "print ''\n",
    "print 'lambda = 0.5 -- k = 30'\n",
    "pred = model35.predict(testCouples)\n",
    "print \"Erreur de validation:\", ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiences sur les données Movie Lens 1M\n",
    "\n",
    "Le dataset Movie Lens 1M contient 1 million d'entrées, données par 6000 utiilisateurs sur 4000 films.\n",
    "\n",
    "Une remarque que l'on peut déjà faire est que la matrice des scores est \"moins sparse\" que celle issue de la base 100k.\n",
    "\n",
    "En effet, pour la base 100k on a 100000 notes pour une matrice 1000x1700, soit un remplissage de 17% de la matrice.\n",
    "\n",
    "Pour la base 1M, on a 1 000 000 de notes pour une matrice 6000x4000, soit 24% de remplissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chargement\n",
    "data, timestamps = loadMovieLens1M()\n",
    "\n",
    "# Récupérer la représentation en liste de triplets\n",
    "couples = getCouplesUsersItems(data)\n",
    "\n",
    "# Séparer en ensemble d'apprentissage et de test\n",
    "trainCouples, testCouples = splitTrainTest(couples,.20)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble d'apprentissage\n",
    "trainUsers = buildUsersDict(trainCouples)\n",
    "trainItems = buildItemsDict(trainCouples)\n",
    "\n",
    "# Supprimer de l'ensemble de test les éléments inconnus en apprentissage\n",
    "toDel = []\n",
    "for i,c in enumerate(testCouples):\n",
    "    if not c[0] in trainUsers:\n",
    "        toDel.append(i)\n",
    "    elif not c[1] in trainItems:\n",
    "        toDel.append(i)\n",
    "testCouples = np.delete(testCouples, toDel, 0)\n",
    "\n",
    "# Reconstruire les dictionnaires pour l'ensemble de test\n",
    "testUsers  = buildUsersDict(testCouples)\n",
    "testItems  = buildItemsDict(testCouples)\n",
    "\n",
    "# taille des données\n",
    "#print len(trainUsers), len(testUsers)\n",
    "#print len(trainItems), len(testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorisation Matricielle\n",
    "\n",
    "Avec un score en apprentissage de 0.82 et de généralisation de 0.85, on peut estimer que le paramètre de régularisation choisi (lambda = 0.2) est raisonnable pour ce problème. La factorisation matricielle est aussi meilleure que celle de la base 100k, probablement parceque la matrice d'apprentissage est moins sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model8 = matrixFactorisation(10, alternate=0, maxIter=1000)\n",
    "model8.fit(trainUsers, trainItems, trainCouples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model8.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model8.predict(testCouples)\n",
    "print ((pred - np.array(testCouples[:,2], float)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Dans ce travail, nous avons implémenté des modèles de filtrage collaboratif que nous avons ensuite chercher à évaluer.\n",
    "Si on a observé qu'un modèle simple de factorisation matricielle pouvait obtenir des résultats concluants, meilleurs qu'une baseline naïve, nous n'avons pu démontré d'intérêt pratiques des modèles avec biais.\n",
    "\n",
    "Cependant, nous avons observés que si nos modèles sans biais avaient des scores similaires en apprentissage qu'en généralisation, ce n'était pas le cas de nos modèles avec biais qui obtiennent de bien meilleurs scores en apprentissage qui ne se traduisent pas en test. On peut en déduire que nous n'avons pas déterminé les bons hyperparamètres pour permettre une bonne généralisation, et que vraisemblablement on peut encore augmenter le score en généralisation de nos modèles avec biais.\n",
    "\n",
    "Enfin, nous n'avons eu le temps ni d'évaluer les modèles avec biais temporel, ni l'influence de la dimension de factorisation matricielle, que l'on peut aussi voir comme une forme de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "class tSNE():\n",
    "    def __init__(self,perp, nIter, lr, moment, dim=2):\n",
    "        self.perp = perp # entre 5 et 50\n",
    "        self.nIter = nIter\n",
    "        self.lr = lr\n",
    "        self.moment = moment\n",
    "        self.dim = dim \n",
    "    def fit(self,data):\n",
    "        nEx = np.shape(data)[0]\n",
    "        # Matrice des distances de ||xi - xj||² #\n",
    "        normx = np.sum(data**2,1)\n",
    "        normx = np.reshape(normx, (1, nEx))\n",
    "        distancex = normx + normx.T - 2 * data.dot(data.T)\n",
    "        # Calcul des sigma ---------------------------------------------------------------#\n",
    "        lperp = np.log2(self.perp)\n",
    "        # initialisation bornes pour la recherche dichotomique #\n",
    "        sup = np.ones((nEx,1)) * np.max(distancex)\n",
    "        inf = np.zeros((nEx,1))\n",
    "        self.sigma = (sup + inf) / 2.\n",
    "        # recherche dichotomique #\n",
    "        stop = False\n",
    "        while not stop:\n",
    "            # Calculer la matrice des p(i|j)\n",
    "            self.pcond = np.exp(-distancex / (2. * (self.sigma**2)))\n",
    "            self.pcond = self.pcond / np.sum(self.pcond - np.eye(nEx),1).reshape(nEx,1)\n",
    "            # Calculer l'entropie de p(i|j)\n",
    "            entropy = - np.sum(self.pcond * np.log2(self.pcond), 0)\n",
    "            # Mise a jour des bornes\n",
    "              # Si il faut augmenter sigma\n",
    "            up = entropy < lperp \n",
    "            inf[up,0] = self.sigma[up,0]\n",
    "              # Si il faut baisser sigma\n",
    "            down = entropy > lperp \n",
    "            sup[down,0] = self.sigma[down,0]\n",
    "            # Mise a jour de sigma et condition d'arrêt\n",
    "            old = self.sigma\n",
    "            self.sigma = ((sup + inf) / 2.)\n",
    "            if np.max(np.abs(old - self.sigma)) < 1e-5:\n",
    "                stop = True\n",
    "                print np.exp(entropy)\n",
    "                print self.sigma.T  \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #initialiser y\n",
    "        self.embeddings = np.zeros((self.nIter+2, nEx, self.dim))\n",
    "        self.embeddings[1] = np.random.randn(nEx, self.dim) * 1e-4\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # p(ij)\n",
    "        self.pij = (self.pcond + self.pcond.T) / (2.*nEx)\n",
    "        np.fill_diagonal(self.pij, 0)\n",
    "        # Descente de Gradient\n",
    "        loss = []\n",
    "        for t in xrange(1,self.nIter+1):\n",
    "            # Matrice des distances \n",
    "            normy = np.sum((self.embeddings[t]**2),1)\n",
    "            normy = np.reshape(normy, (1, nEx))\n",
    "            distancey = normy + normy.T - 2 * self.embeddings[t].dot(self.embeddings[t].T)\n",
    "            # q(ij)\n",
    "            # self.qij = (distancey.sum() + nEx*(nEx-1)) / (1 + distancey)\n",
    "            # np.fill_diagonal(self.qij, 0)\n",
    "            self.qij = 1 / (1 + distancey)\n",
    "            np.fill_diagonal(self.qij, 0)\n",
    "            self.qij = self.qij / self.qij.sum()\n",
    "            # Descente de gradient\n",
    "            yt = self.embeddings[t]\n",
    "            tmpgrad = 4 * ((self.pij - self.qij) / (1 + distancey)).reshape(nEx, nEx,1)\n",
    "            for i in range(nEx):\n",
    "                dy = (tmpgrad[i] * (yt[i]-yt)).sum(0)\n",
    "                self.embeddings[t+1][i] = yt[i] - self.lr * dy + self.moment * (yt[i] - self.embeddings[t-1,i])\n",
    "            l = stats.entropy(self.pij, self.qij, 2).mean()\n",
    "            loss.append(l)\n",
    "            print t,l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tSNE(30,100,1000,0)\n",
    "model.fit(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.shape(model.embeddings)[0] -1\n",
    "plt.figure()\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 0], model.embeddings[t,:,1][data.target == 0], 'o', color=\"blue\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 1], model.embeddings[t,:,1][data.target == 1], 'o', color=\"red\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 2], model.embeddings[t,:,1][data.target == 2], 'o', color=\"cyan\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 3], model.embeddings[t,:,1][data.target == 3], 'o', color=\"magenta\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 4], model.embeddings[t,:,1][data.target == 4], 'o', color=\"yellow\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 5], model.embeddings[t,:,1][data.target == 5], 'o', color=\"black\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 6], model.embeddings[t,:,1][data.target == 6], 'o', color=\"white\")\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 7], model.embeddings[t,:,1][data.target == 7], 'o', color=(0.5, 0.5, 0))\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 8], model.embeddings[t,:,1][data.target == 8], 'o', color=(0,0.5,0.5))\n",
    "plt.plot(model.embeddings[t,:,0][data.target == 9], model.embeddings[t,:,1][data.target == 9], 'o', color=(0.5,0,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Lens 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "fichier = open(\"./model.p\")\n",
    "reco = pkl.load(fichier)\n",
    "fichier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = np.zeros((len(reco.q),10))\n",
    "titles = []\n",
    "for i,q in enumerate(reco.q.keys()):\n",
    "    movies[i] = np.reshape(reco.q[q],10,1)\n",
    "    titles.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tSNE(30,1000,1e3,0)\n",
    "model.fit(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.embeddings[11,:,0],model.embeddings[11,:,1], 'o', color=\"blue\")\n",
    "\n",
    "\n",
    "#y=[2.56422, 3.77284,3.52623,3.51468,3.02199]\n",
    "#z=[0.15, 0.3, 0.45, 0.6, 0.75]\n",
    "#n=[58,651,393,203,123]\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.scatter(z, y)\n",
    "\n",
    "for i, txt in enumerate(titles):\n",
    "    if (np.random.rand() > .9):\n",
    "        plt.annotate(txt.decode('latin-1'), model.embeddings[11,i])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
